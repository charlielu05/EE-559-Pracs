{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Nearest neighbor <br>\n",
    "Write a function that gets a training set and a test sample and returns the label of the training point the closest to the latter.<br>\n",
    "More precisely, write:<br>\n",
    "def nearest_classification(train_input, train_target, x):<br>\n",
    "where<br>\n",
    "• train_input is a 2d float tensor of dimension n × d containing the training vectors, <br>\n",
    "• train_target is a 1d long tensor of dimension n containing the training labels,<br>\n",
    "• x is 1d float tensor of dimension d containing the test vector,<br>\n",
    "and the returned value is the class of the train sample closest to x for the L2 norm.<br>\n",
    "Hint: The function should have no python loop, and may use in particular torch.mean , torch.view ,<br>\n",
    "torch.pow , torch.sum , and torch.sort or torch.min . My version is 164 characters long.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/L2.png\", width= 600, height = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training examples, compare each samples L2-norm to the test data, find min difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 \n",
    "test_tensor = test_input[0]\n",
    "test_label = test_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12921c6a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validate that it is number 7\n",
    "plt.imshow(test_tensor.numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "train_tensor = train_input[0]\n",
    "train_label = train_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12910be10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validate that it is number 5 \n",
    "plt.imshow(train_tensor.numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index location for number equals to 7\n",
    "print ((train_target == 7).nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7321.2207)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2-norm of 7 and 5\n",
    "torch.mean(torch.pow((test_tensor - train_tensor),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5114.3701)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2-norm of 7 and 7\n",
    "torch.mean(torch.pow((test_tensor - train_input[15]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6113.7500)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2-norm of 7 and 1\n",
    "torch.mean(torch.pow((test_tensor - train_input[14]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7855.7261)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.pow((test_tensor - train_input), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow((test_tensor - train_input), 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dif = torch.pow((test_tensor - train_input), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  7321.2207,   8961.5322,   7391.7515,   8337.0576,   7015.3140,\n",
       "          7771.8867,   7209.6172,   8920.5703,   5994.8979,   6549.0410,\n",
       "          7741.6606,   7322.4004,  11048.4473,   9087.2822,   6113.7500,\n",
       "          5114.3701,   6614.7974,   7873.9644,   6822.4683,   5632.9590,\n",
       "         10952.7285,  10022.4785,   6001.4731,   8125.7769,   7636.8291,\n",
       "         11408.8760,   5093.8203,  10203.3389,   7684.5205,   6386.1133,\n",
       "          7181.1328,   9874.1133,   7095.3403,   5533.8799,   8011.8457,\n",
       "          6853.4922,  10107.4609,   8980.8906,   5616.8765,   7305.5508,\n",
       "          5825.6519,   7895.8994,   4426.8813,   5742.9897,   6145.6328,\n",
       "          4511.5562,   7146.6123,   7796.1621,   6488.5483,   8922.8906,\n",
       "          7466.9526,  11898.7891,   3969.4695,   7238.6860,   4296.1299,\n",
       "          7619.7183,  11669.1426,   4692.1211,   9903.1621,   8285.6094,\n",
       "          9772.4258,   7888.8799,  10051.5361,  12483.2256,   8810.2295,\n",
       "          8225.2568,   9451.9502,   6797.7119,   9515.1328,  10146.0957,\n",
       "          7712.1890,   5579.5229,   5758.4819,   8575.8457,   7421.7896,\n",
       "          9159.0859,   8970.8535,   8015.9297,   7780.4478,   7103.9517,\n",
       "          8399.9697,   8855.1045,  11368.9961,   9194.1611,   2865.7856,\n",
       "          7678.9492,   7915.6353,   5078.7910,  10468.5127,   6559.8521,\n",
       "          9775.3574,   5433.0713,   6275.5767,   9259.0508,   7255.9194,\n",
       "          9750.8916,   3778.9146,   7440.8750,   7111.6162,   6999.7310,\n",
       "          7457.0601,   4117.9746,   6184.3506,   2515.2092,   6104.2744,\n",
       "          8367.6904,   9549.0264,   9176.8164,   9895.2041,   7672.3267,\n",
       "          7985.5776,  10682.6621,   5833.3047,   9518.8125,  11257.3613,\n",
       "          4966.1899,   6182.7500,  10933.1162,   8489.1904,  11759.4678,\n",
       "          7242.6543,  14912.5127,   9488.7158,   5085.8838,   5543.2974,\n",
       "          8068.5347,  10029.6172,   7528.3433,   7295.3306,   9814.8135,\n",
       "          7071.6733,   8275.2969,   5916.5972,   3852.7666,   8347.2373,\n",
       "          9055.9707,   7519.9351,   8370.7129,   9529.2842,   7083.4185,\n",
       "          5279.1006,   7556.2144,   5497.8062,   7729.6084,   7206.9463,\n",
       "          7298.9731,   8873.6670,   8136.8125,   5195.5894,  10188.8057,\n",
       "          8357.7578,   8358.2861,   6620.9731,   5938.5127,   5280.5205,\n",
       "          9201.4258,   9458.1826,   8953.1904,   7193.6250,   8085.0767,\n",
       "          7143.5918,  11124.8926,   6499.9414,   8819.7637,  10656.7637,\n",
       "         10409.9463,   6261.1992,   4109.5942,   6287.2222,   9730.0557,\n",
       "          6354.8867,   8987.9014,   5179.1748,   7005.5806,   6031.1582,\n",
       "          7004.0381,   6674.1069,   7479.4897,   8155.0674,   9436.5996,\n",
       "          6739.7500,   6454.4004,   9073.7617,   4038.0930,   6459.1099,\n",
       "          5371.8086,   8857.6318,  13219.8057,   7930.1758,   9259.0322,\n",
       "         10739.0781,   6737.8101,   9892.2197,   5824.0063,   8613.3125,\n",
       "          5108.9248,   7221.1416,   7968.3647,   9225.4512,   6575.3662,\n",
       "          6705.4326,   8136.2119,   7794.4746,   7331.9492,   7280.1060,\n",
       "          8548.3955,  10765.3779,   5795.3711,   6795.5229,  11909.5820,\n",
       "          6620.8750,   7432.3369,   6188.9517,  12046.0625,   5668.6021,\n",
       "          9474.7207,   8671.3457,   6237.3369,   9219.9004,   8345.3916,\n",
       "         11770.7305,  10827.9639,  11965.7295,   2941.8062,   7017.3979,\n",
       "          9295.1543,   6590.8330,   4969.8369,   8369.7139,  10955.7236,\n",
       "          3946.8303,   7468.8379,   6366.6265,  10155.2324,   8632.9736,\n",
       "          8809.0000,   7229.1133,   6152.6401,   8233.0215,   6899.3140,\n",
       "          6437.8521,   9565.4795,  10911.9424,   2224.5752,  11606.3721,\n",
       "          9077.7402,   8550.8701,   4927.1606,   7632.3086,  10771.3076,\n",
       "          7245.3076,   6992.4399,   9695.1660,  11592.2041,   8558.0830,\n",
       "          9488.0127,   6738.2524,   5675.2246,   3720.3738,   6505.6479,\n",
       "          6245.9517,   8498.9014,   8005.1646,   2652.9426,   7549.0498,\n",
       "          8037.8931,   9958.0967,   4735.3472,   6030.1670,   7759.8403,\n",
       "          5704.7642,   6291.4351,   5634.8330,   6796.0903,   8968.9033,\n",
       "          5874.8569,   5697.4937,  10967.9219,   8553.5488,   8068.0674,\n",
       "          4048.1008,   9631.1348,   6467.2183,  13422.3994,  10236.8027,\n",
       "          5095.1035,  13461.0742,   6242.2944,   2541.4387,   5577.4233,\n",
       "          6785.8774,   7690.2944,   7003.0537,  13454.6074,  11470.9668,\n",
       "          7704.3828,   9610.7764,   5217.7500,   9744.4180,  10368.1787,\n",
       "          6994.7412,   2703.7781,   9127.9912,  11862.3623,   5859.7578,\n",
       "          4576.9287,   9407.4697,   3342.6772,  11483.4863,   7773.3853,\n",
       "          5751.7256,   8526.2324,   8705.6445,   6060.5254,   7346.2437,\n",
       "          8158.7065,   7664.4858,  10448.6719,   8319.4287,   5568.1045,\n",
       "          9156.5322,   7318.9028,   7096.9453,   9609.9932,   5065.2500,\n",
       "         11546.1973,   8596.0234,   8447.3340,   9496.6055,   6465.4756,\n",
       "          8027.2681,   7982.7397,   8375.5000,   9808.3330,   7767.7017,\n",
       "          6214.2549,   6274.8623,   3516.3726,   5143.1250,  11294.1045,\n",
       "          7565.1250,  10058.6133,   6107.9067,   8252.2344,   6868.5957,\n",
       "          8299.9580,   5751.7847,  10469.3906,   9598.1582,   2507.2437,\n",
       "          7124.3892,   8309.6582,  10928.8789,   4008.6313,   6451.0420,\n",
       "          9096.9629,   7284.2729,   8212.7803,   5951.3433,  10087.5918,\n",
       "         11494.5879,   8681.6992,   5566.1338,   7200.0332,   5628.0713,\n",
       "          9652.4092,   5713.9922,   9501.3994,   6740.6060,   5883.6724,\n",
       "          5867.1338,   3343.6187,   6001.7437,   6375.8174,   9297.7324,\n",
       "         11254.1123,   8189.9121,   3731.5435,  10550.1992,   6807.7563,\n",
       "          8043.2769,   9782.1572,   7727.3545,   5007.4580,   7116.4043,\n",
       "         13330.4424,  10320.0244,   3592.6606,   6276.0444,   4704.4209,\n",
       "          9881.1533,   9579.3672,   9794.9131,   8994.5420,   7185.0444,\n",
       "          9122.3408,   7242.5381,   8327.2432,   5824.1123,  12813.7490,\n",
       "          9375.2363,   7147.6265,   5577.3506,   7231.4438,   9062.6162,\n",
       "          9058.2070,   7096.8076,   7251.5181,   6060.7871,   5399.0981,\n",
       "         10495.0381,   4795.6787,   5049.8569,   4804.7603,   8758.3359,\n",
       "          8739.3174,   7060.9683,   5376.5356,   5802.9390,   4385.3726,\n",
       "          5346.2539,   8903.3262,   3432.5676,   5300.0957,  10066.7715,\n",
       "          7539.4131,   8658.6035,  13180.0527,   7554.1528,  13431.2549,\n",
       "          8183.2373,   8385.2041,   6740.9692,   8067.8838,   6359.0039,\n",
       "         11850.4053,   4949.2461,   6224.6021,   8384.2705,   7764.4326,\n",
       "         13584.3965,   4053.4822,   6047.8535,   8311.7656,   6480.1328,\n",
       "          8213.9336,   6282.5981,   8419.1152,  10188.7764,   8371.6670,\n",
       "          6463.3877,   9190.7168,   9966.5176,  11008.0996,   5402.1431,\n",
       "          8061.1938,   7339.9937,   8770.7285,  13246.4639,   7288.3228,\n",
       "          6429.0610,   5634.8418,  11996.8594,   5832.1133,  12539.5596,\n",
       "         10131.0254,   6259.2769,   4871.2383,   7208.0117,   8364.3545,\n",
       "          7031.0088,   6061.2769,   6961.1685,  11301.4619,   8185.0664,\n",
       "          7779.3354,   6930.8354,   9171.6660,   5053.2104,   8461.0283,\n",
       "          9290.6455,   6875.3149,   5806.3457,   6596.0332,   5668.0854,\n",
       "          8911.0723,   8640.0820,   6635.1069,   9197.1465,   9254.1494,\n",
       "          9397.8242,   8054.7715,   7816.8330,   9492.5527,   8494.7158,\n",
       "         10425.4863,   9037.1758,   6515.4849,   7576.8403,   8297.3818,\n",
       "          4907.4502,   6652.7056,   6419.9644,   8573.0381,  11852.4053,\n",
       "          3418.9885,   8352.1338,   8306.5703,   7989.6299,   7724.3188,\n",
       "          6945.9463,   6537.9209,   7216.4146,   6550.7651,   5759.5483,\n",
       "         12219.3076,   9234.0742,   8647.7295,   5638.1567,  10926.4971,\n",
       "          8245.9219,   6526.5918,   1854.6608,   6478.1235,  10381.3486,\n",
       "          6522.1172,  12604.8174,   7075.1035,  12100.0947,   7617.1860,\n",
       "          6521.3994,   7582.9526,   8777.8555,   7867.0576,   8916.2109,\n",
       "          8565.3965,   6573.9224,   6214.3213,   7096.5371,   9439.9658,\n",
       "          6824.0024,   6146.1338,   9776.2500,   7551.4707,  10204.4307,\n",
       "          8113.5151,   9035.2266,   7783.0933,   8753.4365,   7733.1123,\n",
       "          7377.9785,   9741.5088,   7384.3838,   8027.7588,  10933.9951,\n",
       "          9062.5674,   6956.1826,   8537.8203,   6250.9707,   8971.7432,\n",
       "          6728.2627,   8544.3633,   4778.1685,  10736.3877,   6233.0410,\n",
       "          9801.8613,   6593.8267,   4004.8022,   7171.9478,   8202.9141,\n",
       "          8661.0430,   8376.3838,   7022.4883,   8026.2754,   5744.4619,\n",
       "          8501.3535,   6396.2002,  10516.1240,   8474.5879,   7042.6709,\n",
       "          6430.1660,   8441.3887,   9383.1572,   7064.0420,   6134.9653,\n",
       "          5898.0498,   5281.1313,   8087.7920,   7265.4224,   6236.1289,\n",
       "          6154.6226,  10179.8516,   6991.0278,   8213.2461,   8396.7881,\n",
       "          6362.2769,   9954.5215,   9901.6777,  10672.3750,   5193.2026,\n",
       "          5698.1758,   6872.4834,   8723.1074,   9752.6777,   5275.7998,\n",
       "          8048.4653,  11037.4277,   8842.0840,   5319.9517,   8259.1406,\n",
       "          7821.0166,   7401.6899,   8963.1836,   6935.6035,   7681.1504,\n",
       "          7842.6147,   8922.9932,   5631.4043,   6237.1455,   8831.2432,\n",
       "         12489.7891,   4950.5972,  10192.2910,  10159.7295,   2981.7361,\n",
       "          8295.8594,   7667.6108,   6976.4312,   7468.0972,   7274.2192,\n",
       "          7023.9951,   7467.2090,   5807.7935,   8396.8125,   4860.4121,\n",
       "          8064.1465,   2869.7231,   7762.6938,   5994.5869,   9687.7598,\n",
       "          8399.7236,   8243.1279,  10215.1172,   9666.4512,   5867.0903,\n",
       "          8487.9668,   7871.3164,   7143.6772,   6532.0166,   9681.1250,\n",
       "          6538.2856,   5548.4463,   7103.1709,   6843.0381,   3243.1289,\n",
       "          7409.6211,   9546.0391,   8904.4590,   9350.0840,   7753.3472,\n",
       "          7370.9312,   8043.6890,   8198.8262,   7561.5078,   6258.8125,\n",
       "         11957.6953,   6927.1069,   9078.5615,   9516.6113,  11180.8672,\n",
       "         12096.8359,   8735.5293,  10029.3701,  10291.2451,   6644.5396,\n",
       "          9128.6777,   6002.3252,   7053.8379,   5943.1172,   9346.0615,\n",
       "          4699.7192,  10909.8037,   6277.3994,   4782.4961,   9407.6680,\n",
       "          9150.7256,   4516.0269,   5421.3315,   7678.6416,  11037.9346,\n",
       "          8331.3906,   8721.1582,   7827.9043,  10049.4922,   7064.3813,\n",
       "          8046.6924,   7672.0088,   6970.9873,   8184.0894,   9488.1338,\n",
       "          4665.6401,  10569.6738,   8989.1084,   3998.5586,   9372.6250,\n",
       "          9267.5156,   8511.5859,   4997.4785,   7819.5791,  10410.1172,\n",
       "          6485.1328,   8957.0654,   9627.7402,  12655.5254,   9610.2158,\n",
       "          9100.0010,   9275.7949,   7505.5117,   6566.8560,  12809.0596,\n",
       "          6126.6235,  10948.3545,   9490.6055,   5718.1367,   4512.8433,\n",
       "         10132.0928,   7920.9146,   4850.0972,   5571.4731,   8112.7793,\n",
       "          3815.0154,   8229.1758,  11515.3340,  12620.6641,   9949.6260,\n",
       "          9671.0195,   6680.8853,   9935.6377,   5715.8813,  10226.5879,\n",
       "          7534.3354,   4945.8076,   7473.1567,  13792.7041,   6132.6660,\n",
       "         11821.4834,   6434.5166,   8243.2266,   8433.4639,   6885.4556,\n",
       "          5173.3418,   9474.6016,  10111.7363,   5696.1479,   5136.9004,\n",
       "          6366.2373,  12644.1406,  10267.6172,   9763.8057,  10176.0459,\n",
       "          7429.8008,   7716.9272,   6931.3672,   7127.1797,   8751.0098,\n",
       "          9380.5752,   9940.9971,   8567.3467,   8616.4902,   9837.0332,\n",
       "          5975.4160,   8662.6045,   8331.3340,   1761.1046,  10205.0527,\n",
       "          6499.6875,   7479.3047,   8129.8867,   5618.4058,   8908.4492,\n",
       "          6434.6274,  13362.8701,   5607.1353,   9039.6328,   6160.4985,\n",
       "          9080.2881,   5182.9707,  11918.9678,   7356.6631,   7948.2881,\n",
       "         11088.0635,   9163.1455,   9797.9385,   4906.1328,  10536.7529,\n",
       "          6327.0586,   5546.4580,   7130.8916,   3328.4236,   8413.3203,\n",
       "          7695.1772,  10260.1162,   8303.8818,   6102.0308,   8119.6084,\n",
       "          7417.2222,   7139.1938,   9535.6465,   5599.5981,   8470.1719,\n",
       "         10842.3662,   8203.6582,   6306.4517,   8869.6807,  12709.3594,\n",
       "          9276.4043,   8450.3213,   8447.6670,   8227.7734,   3030.8611,\n",
       "          6066.6274,   9618.7637,   4895.6123,   7696.8433,   4533.7769,\n",
       "         11617.6963,   9193.2920,   5756.2549,   9644.9619,  10188.3379,\n",
       "         10140.1758,   9278.9258,   6791.7090,   9120.7998,   6957.7871,\n",
       "          8814.1953,   8830.7402,   9470.7754,   5564.4453,   4582.1660,\n",
       "          9353.6338,  10846.4619,   8915.2344,   8213.5088,   8240.3828,\n",
       "          9363.5557,  10738.0918,   6486.4160,   8827.4014,  10624.7734,\n",
       "          7081.8916,   7526.7295,   7762.4683,   4305.4326,   5349.9453,\n",
       "          6764.2651,   5940.7769,   8839.4043,   6776.7642,  10365.5371,\n",
       "         10582.2217,   8682.9756,   8698.6533,   8589.7656,   5291.5254,\n",
       "          8456.9746,   7388.6069,   9438.0664,   2424.2346,   8761.9805,\n",
       "          6985.4492,   8376.9902,  11773.2090,   9978.8535,  11698.8447,\n",
       "         12105.6221,   7322.0806,   8022.3101,   6151.0278,   7174.1035,\n",
       "          3575.5471,   9458.9082,   9322.0762,   4443.4424,   6717.1006,\n",
       "         10419.8721,  10797.4961,   5830.5508,   4526.4233,   9726.5410,\n",
       "          8686.5498,   8455.8623,   6656.6265,  10691.6328,   7657.6084,\n",
       "         12296.1494,   8909.1377,   6708.9185,   4154.1685,   6559.5894,\n",
       "          5655.8838,   8518.0586,   8762.5781,   9061.3984,   4937.3877,\n",
       "          8654.0371,   8611.8320,  11773.7207,   7575.2476,  11732.7383,\n",
       "          9018.5088,   5627.4028,   7817.4106,  10443.7432,   7924.3110,\n",
       "          6209.4058,  11283.1338,  10390.8164,   7625.3774,   8560.4766,\n",
       "          8033.7856,   6959.0024,   5927.5063,   9004.7822,  11284.2666,\n",
       "          9843.5830,   9289.1055,   9596.0654,   2617.7935,   5172.4912,\n",
       "          6095.7910,   9503.1992,   8034.7959,   5582.7080,   3955.1033,\n",
       "          7883.3799,   5217.5586,   6701.5435,   8506.6924,   6755.1069,\n",
       "          7384.7549,   7024.3140,   7134.5728,   7218.6367,  13211.4248,\n",
       "          5611.7881,   9926.3584,   7055.2207,   7843.8340,   9195.6211,\n",
       "          6150.6211,   8234.4551,   7379.2349,   7316.1724,   4290.8789,\n",
       "          7302.6006,   9521.2617,   6512.4897,   9511.9844,  10134.0010,\n",
       "         10828.1846,   7534.0117,   2185.6914,   8286.2334,   7241.6265,\n",
       "          8690.6182,   7572.6299,   6663.6875,   5459.7588,   4838.0269,\n",
       "          9160.2002,   5853.8101,   8462.0742,   8338.6318,   9299.6758,\n",
       "         11025.9834,   8123.8354,   7883.1108,   5997.0420,   7732.6670,\n",
       "          6482.4922,   6520.6567,   6527.8838,   8151.4297,   8830.7510,\n",
       "          6204.3125,  10475.4453,   5659.4453,   7766.5396,  11071.1318,\n",
       "          9595.7988,   7734.6084,   9483.1582,   7112.4668,   3763.3572,\n",
       "          8227.4902,   8101.6733,   8754.0947,   8023.6606,   8190.0791])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the mean value using dim = 1 \n",
    "torch.mean(tensor_dif, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1761.1046), tensor(773))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the minimum tensor value and index location: value, index\n",
    "torch.min(torch.mean(torch.pow((test_tensor - train_input), 2), 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_classification(train_input, train_target, x):\n",
    "    # find the minimum tensor value and index location: value, index\n",
    "    value, index = torch.min(torch.mean(torch.pow((x - train_input), 2), 1), 0)\n",
    "    #print(train_target[index].data.tolist())\n",
    "    \n",
    "    return train_target[index].data.tolist() # we need to convert the tensor to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "val= nearest_classification(train_input, train_target, test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Error estimation <br>\n",
    "\n",
    "Write a function <br>\n",
    "\n",
    "def compute_nb_errors(train_input, train_target, test_input, test_target,\n",
    "                      mean = None, proj = None): <br>\n",
    "where<br>\n",
    "• train_input is a 2d float tensor of dimension n × d containing the train vectors, <br> \n",
    "• train_target is a 1d long tensor of dimension n containing the train labels,<br>\n",
    "• test_input is a 2d float tensor of dimension m × d containing the test vectors, <br>\n",
    "• test_target is a 1d long tensor of dimension m containing the test labels,<br>\n",
    "• mean is either None or a 1d float tensor of dimension d,<br>\n",
    "• proj is either None or a 2d float tensor of dimension c × d ,<br>\n",
    "\n",
    "that subtracts mean (if it is not *None*) from the vectors of both train_input and test_input, apply the operator proj (if it is not*None*) to both, and returns the number of classification errors using the 1-nearest-neighbor rule on the resulting data.<br>\n",
    "Hint: Use in particular torch.mm . My version is 487 characters long, and it has a loop (the horror!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(train_input, train_target, test_input, test_target, mean = None, \n",
    "                      proj = None):\n",
    "    \n",
    "    if mean != None:\n",
    "        train_input = (train_input - mean)\n",
    "        test_input = (test_input - mean)\n",
    "    if proj != None:\n",
    "        train_input = train_input.mm(torch.transpose(proj, 0, 1))\n",
    "        test_input = test_input.mm(torch.transpose(proj, 0, 1))\n",
    "    \n",
    "    nb_errors = 0\n",
    "    \n",
    "    for i in range(test_input.shape[0]):\n",
    "        if test_target[i].tolist() != nearest_classification(train_input, \n",
    "                                                             train_target, test_input[i]):\n",
    "            nb_errors += 1\n",
    "    \n",
    "    print(nb_errors)\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "errors = compute_nb_errors(train_input, train_target, test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 PCA <br>\n",
    "Write a function <br>\n",
    "\n",
    "def PCA(x): <br>\n",
    "\n",
    "where x is a 2d float tensor of dimension n × d , which returns a pair composed of the 1d <br>mean vector of dimension d and the PCA basis, ranked in decreasing order of the eigen-<br>values, as a 2d tensor of dimension d × d. <br>\n",
    "\n",
    "Hint: The function should have no python loop, and use in particular torch.eig , and torch.sort . My version is 275 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [ 513.1667,    0.0000],\n",
       "         [ 554.7198,    0.0000],\n",
       "         [-324.6695,    0.0000],\n",
       "         [ 197.7800,    0.0000],\n",
       "         [-128.6437,    0.0000],\n",
       "         [ 106.2060,    0.0000],\n",
       "         [  19.9167,   17.6108],\n",
       "         [  19.9167,  -17.6108],\n",
       "         [ -58.4233,    0.0000],\n",
       "         [  34.4516,    0.0000],\n",
       "         [  -0.4726,    0.0000],\n",
       "         [   0.0241,    0.5667],\n",
       "         [   0.0241,   -0.5667],\n",
       "         [   0.7201,    0.0000],\n",
       "         [   5.2827,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000],\n",
       "         [   0.0000,    0.0000]]), tensor([]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eig(test_tensor.view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EE-559",
   "language": "python",
   "name": "ee559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
